{"data":{"site":{"siteMetadata":{"title":"Bruno Opsenica's Blog","author":"Bruno Opsenica"}},"markdownRemark":{"id":"2d3340b4-770f-5e04-a624-a4c1a81127fe","excerpt":"Introduction Recently, I’ve been building a small renderer using the cross-platform rendering library  bgfx  as a learning exercise. To start, I decided to load…","html":"<h2>Introduction</h2>\n<p>Recently, I’ve been building a small renderer using the cross-platform rendering library <a href=\"https://github.com/bkaradzic/bgfx\">bgfx</a> as a learning exercise. To start, I decided to load in some <a href=\"\">glTF</a> models and write a basic PBR shader using the Cook-Torrance model described in the <a href=\"\">glTF specification</a>. The implementation is incomplete and it renders the material using only punctual lights. It’s a single vertex + fragment shader. I loaded up the <a href=\"\">FlightHelmet</a> model, added two lights on each side, started up my program and saw this:</p>\n<p>[IMAGE OF SHIT LDR IMAGE]</p>\n<p>You can see that the lit areas are basically just the color of the light sources, with ugly, uniform splotches removing all of the detail we’d expect to get from the normal maps and albedo textures. This is not unexpected. It may not be obvious why this isn’t a bug and why we’d expect our fragment shader to produce such an ugly image, so let me quickly explain.</p>\n<h3>Physically Based Lighting</h3>\n<p>In physically based rendering, objects are rendered using <em>physically based units</em> from the fields of <a href=\"\">Radiometry</a> and <a href=\"\">Photometry</a>. The <a href=\"\">rendering equation</a> is meant to produce the <a href=\"\">radiance</a> for each pixel. This means, for instance, that the lights are using the photometric unit of <a href=\"\">lumens</a>, and both are set to emit 800 lm, but it could be much higher. The sun, a directional light, illuminates the earth’s surface with ~120,000 <a href=\"\">lux</a>. This means that when we solve the rendering equation for these objects, we’re going to end up with values that are effectively unbounded and we may end up with radiance values that differ by several orders of magnitude in the same frame. Additionally, all of our calculations are taking place in linear space — that is to say, that an RGB value corresponding to (1.0, 1.0, 1.0) corresponds to half as much radiance as a value of (2.0, 2.0, 2.0).</p>\n<p>This is a problem however, because (most) of our displays work differently. Displays expects our frame buffer to contain RGB values in the <a href=\"\">sRGB color space</a> that are between 0 and 1, with (1.0, 1.0, 1.0) corresponding to white.<sup><a href=\"#note_1\">1</a></sup> So any RGB that our fragment shader produces are clamped to [0, 1] when they are written to the 32-bit back buffer. That’s why everything appears to be basically white!</p>\n<h2>Tone Mapping</h2>\n<p>The solution then, is to take our physical, unbounded HDR values and map them first to a LDR linear space [0, 1], and then finally apply <a href=\"\">gamma correction</a> to produce the sRGB value our displays expect.</p>\n<p>The way we perform the first step, mapping the HDR values produced by our fragment shader to linear LDR values is usually called <em>Tone Mapping</em>. There are a few different ways to perform it, and the way you choose to do it will have an effect on the “look” of your final frame, but the basic steps are:</p>\n<ol>\n<li>Render your scene into a framebuffer that supports HDR values i.e. a floating point buffer with 16-bits or 32-bits per channel. Which one you choose is (as always) a tradeoff between precision and memory, but for my scene I’m going to go with a RGBA32F framebuffer. Make sure your output is in linear space.</li>\n<li>\n<p>In a separate render pass, produce an LDR buffer by:</p>\n<ol>\n<li>Scale the input using the exposure of the image to obtain a “calibrated” RGB value</li>\n<li>Scale the calibrated value using a Tone Curve, inputting either the RGB or the Luminance value of the fragment</li>\n<li>Apply gamma correction to the scaled value and write this result to the back buffer.</li>\n</ol>\n</li>\n</ol>\n<p>If you’re anything like me, you are probably asking yourself:</p>\n<ul>\n<li>How do I actually obtain the “calibrated” fragment value? What does calibration even mean in this context?</li>\n<li>What is a Tone Curve and which one should I use?</li>\n<li>What is the difference between using the luminance value as input to the tone curve vs the RGB value?</li>\n</ul>\n<p>I’m going to tackle these one at a time, focusing first on calibration. Once we’ve made our choices, we’ll revisit this task list and go into implementation.</p>\n<h3>Exposure</h3>\n<p>When a human eye views a scene, it will naturally dilate to adjust to the amount of light reaching it. Similarly, photographers have several ways to control the amount of light reaching the sensors, such as the aperture size (f-stop) and shutter speed. In photography, these controls correspond to the <a href=\"\">exposure value</a>, or $EV$, which is a logarithmic representation of the luminance. Increasing the $EV$ by a value of +1 results in a doubling of the luminance. I won’t spend too much time on $EV$ as I can’t s</p>\n<p>In our context, the exposure linearly scales our the scene luminance to simulate how much light is actually hitting the sensor. It’s up to us to actually provide the exposure value however, and there are a few different ways to do so:</p>\n<ul>\n<li>Modelling a physical camera to calculate the <a href=\"\">exposure value</a> based off artist controlled parameters like f-stop</li>\n<li>Using the scene luminance to calculate an average luminance, which we then use to derive the exposure</li>\n</ul>\n<p>We’re going to follow the second method since this allows us to find the exposure automatically. There are actually quite a few different ways to perform this calculation, many of which are explained in this excellent post by <a href=\"https://knarkowicz.wordpress.com/2016/01/09/automatic-exposure/\">Krzysztof Narkowicz</a>. I’m going to use the method described in <a href=\"https://media.contentapi.ea.com/content/dam/eacom/frostbite/files/course-notes-moving-frostbite-to-pbr-v2.pdf\">Lagard and de Rousiers</a> (pg. 85). First we calculate the $EV_{100}$, the exposure value</p>\n<p>$$\nEV<em>{100} = \\log</em>2{\\frac{L_{avg}S}{K}}\n$$</p>\n<p>$$\n\\text{Exposure} = \\frac{\\text{key value}}{L_{avg}}\n$$</p>\n<p>where $L_{avg}$ is the average scene luminance.</p>\n<p>We’ll need to choose a key value. The most common one I’ve seen is <a href=\"\">“middle gray”</a>, which corresponds to a gray that is <em>perceptually</em> half as bright as white. In reality, the human eye’s response to light is non-linear, and so the gray only reflects 18% as many photons, which gives us a luminance value of 0.18:</p>\n<p>$$\n\\text{Exposure} = \\frac{0.18}{L_{avg}}\n$$</p>\n<p>Our final scaled color is then simply:</p>\n<p>$$\nc<em>{rgb}^\\prime = \\text{Exposure} \\times c</em>{rgb}\n$$</p>\n<p>Note that this value is still not clamped to [0, 1], and so we will still potentially get a lot of clipping.</p>\n<p>Additionally, we haven’t discussed how to actually calculated the average luminance. There are two primary ways to do so:</p>\n<ul>\n<li>Use a geometric average, obtained by repeatedly downsampling the luminance of our HDR image similar to how we would create a mip map chain.</li>\n<li>Create a histogram of some static luminance range.</li>\n</ul>\n<h3>Tone Curves</h3>\n<p>Once we have scaled our scene luminance by the exposure, we can now apply a <em>tone curve</em>. A tone curve is literally just a function that takes our exposed color value (that we obtained above) to</p>\n<h2>Notes</h2>\n<ol>\n<li>This is not true for HDR displays. Those displays use other color spaces that newer games are starting to support. [LINK]</li>\n</ol>\n<!-- I've read about many different techniques in computer graphics on blogs, twitter and my copy of [Real Time Rendering](), but I've rarely actually gone through and implemented any of them. My previous attempts to start had be -->","frontmatter":{"title":"Tone Mapping using a Luminance Histogram","date":"March 28, 2019","description":"A guide to adding tone mapping to your physically based renderer"}}},"pageContext":{"isCreatedByStatefulCreatePages":false,"slug":"/tone_mapping/","previous":null,"next":null}}